<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding">
    <meta name="keywords" content="Multimodal, LLM, 3D Generation, 3D Understanding, 3D Edit">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>ShapeLLM-Omni</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <style>
       
        .results-carousel {
            display: flex;
            flex-wrap: wrap;         
            justify-content: center; 
            align-items: flex-start; 
            gap: 20px;              
           
            
          }
          .caption-text {
            text-align: center;
            margin-bottom: 10px;
            font-weight: bold;
        }

		.render_wrapper {
            position: relative;
            height: 300px;
            
            
        }
   
		.render_div {
			position: absolute;
			top: 0;
			left: 0;
		}

       
       
    </style>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <!-- model viewer -->
    <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/3.1.1/model-viewer.min.js"></script>
    
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <!-- <div class="has-text-centered">
                            <img src="static/images/logo.png" alt="Logo" style="max-height: 100px;">
                        </div> -->
                        <h1 class="title is-1 publication-title">ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding</h1>    
                        <div class="is-size-5 publication-authors">
                            <!-- Paper authors -->
                            <span class="author-block">
                                <div class="is-size-8 publication-authors">
                                    <span class="author-block">
                                        <a href="https://jamesyjl.github.io/">Junliang Ye</a><sup>*1,3</sup>,
                                    <span class="author-block">
                                        <a href="https://thuwzy.github.io/">Zhengyi Wang</a><sup>*1,3</sup>,
                                    <span class="author-block">
                                        <a href="https://zhaorw02.github.io/">Ruowen Zhao</a><sup>*1</sup>,
                                </div>
                                <div class="is-size-8 publication-authors">
                                  <span class="author-block">
                                      Shenghao Xie<sup>2</sup>,
                                  <span class="author-block">
                                    <a href="https://ml.cs.tsinghua.edu.cn/~jun/index.shtml">Jun Zhu</a><sup>1,3</sup>
                                  </span>
                                </div>
                            <div class="is-size-8 publication-authors">
                                <sup>1</sup>Tsinghua University,
                                <sup>2</sup>Peking University, <sup>3</sup>ShengShu
                            </div>
                            (*Equal Contribution)
                            
                        </div>
                            <div class="column has-text-centered">
                                <div class="publication-links">

                                    <span class="link-block">
                                        <a href="https://arxiv.org/abs/2506.01853" class="external-link button is-normal is-rounded is-dark" href="">
                                            <span class="icon">
                                                <i class="ai ai-arxiv"></i>
                                            </span>
                                            <span>arXiv</span>
                                        </a>
                                    </span>
                                    <!-- Github Link. -->
                                    <span class="link-block">
                                        <a href="https://github.com/JAMESYJL/ShapeLLM-Omni" class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fab fa-github"></i>
                                            </span>
                                            <span>Code</span>
                                        </a>
                                    </span>
                                    <!-- Model Weight Link. -->
                                    <span class="link-block">
                                        <a href="https://huggingface.co/yejunliang23/ShapeLLM-7B-omni" class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <svg class="svg-inline--fa fa-database fa-w-14" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="database" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" data-fa-i2svg=""><path fill="currentColor" d="M448 73.143v45.714C448 159.143 347.667 192 224 192S0 159.143 0 118.857V73.143C0 32.857 100.333 0 224 0s224 32.857 224 73.143zM448 176v102.857C448 319.143 347.667 352 224 352S0 319.143 0 278.857V176c48.125 33.143 136.208 48.572 224 48.572S399.874 209.143 448 176zm0 160v102.857C448 479.143 347.667 512 224 512S0 479.143 0 438.857V336c48.125 33.143 136.208 48.572 224 48.572S399.874 369.143 448 336z"></path></svg>
                                            </span>
                                            <span>Checkpoint</span>
                                        </a>
                                    </span>
                                    <!-- Datasets -->
                                    <span class="link-block">
                                        <a href="https://huggingface.co/datasets/yejunliang23/3D-Alpaca" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-database"></i>
                                        </span>
                                        <span>Dataset</span>
                                        </a>
                                    </span>

                                    <!-- Demo -->
                                    <span class="link-block">
                                    <a href="https://huggingface.co/spaces/yejunliang23/ShapLLM-Omni" target="_blank"
                                    class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="fas fa-laptop-code"></i>
                                    </span>
                                    <span>Demo</span>
                </a>
                </span>
                                </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <!-- <section class="hero is-light is-small">
        <h2 class="title is-3" style="text-align: center;"> Demo Video </h2>
        <div class="columns is-centered has-text-centered">
          
          <div class="column is-full-width">
            <div class="column">
              <video id="teaser" autoplay muted loop playsinline width="1350" height="600">
                <source src="demo.mp4" type="video/mp4">
              </video>
              <p>
                <strong>All of the meshes above are generated by DeepMesh.</strong> 
                DeepMesh can generate high-quality meshes conditioned on the given point cloud by auto-regressive transformer.
              </p>
            </div>
          </div>
        </div> -->

        


        
    </section>
    <script>
        window.addEventListener("DOMContentLoaded", function() {
          var video = document.getElementById("teaser");
          video.playbackRate = 0.8; 
        });
      </script>
    
    <!-- <section class="hero is-light is-small">
        <div class="columns is-centered has-text-centered">
          
            <div class="column is-full-width">
              <h2 class="title is-3" style="text-align: center;">Animation of Mesh Generation</h2>
                <p style="text-align: center;">
                    The following video shows an animation of the mesh generation process. We generate all faces of mesh sequentially.
                </p>
              <div class="column">
                
                  <video id="animation" autoplay muted loop playsinline width="1350" height="600">
                  <source src="output.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
    </section> -->
    <!-- <script>
        window.addEventListener("DOMContentLoaded", function() {
          var video = document.getElementById("animation");
          video.playbackRate = 0.75; // 0.5 倍速播放
        });
      </script> -->


   <!-- Paper abstract -->
    <div class="column content">
    <video id="matting-video" autoplay muted controls loop playsinline height="100%">
      <source src="static/images/demo.mp4"
	      type="video/mp4">
    </video>
    <section class="hero is-light is-small">
        <div class="container is-max-desktop">
            <h2 class="title is-3" style="text-align: center;">Abstract</h2>
            <div class="content has-text-justified">
                <p>
                Recently, the powerful text-to-image capabilities of GPT-4o have led to growing appreciation for native multimodal large language models. However, its multimodal capabilities remain confined to images and text. 
                Yet beyond images, the ability to understand and generate 3D content is equally crucial. To address this gap, we propose ShapeLLM-Omni—a native 3D large language model capable of understanding and generating 
                3D assets and text in any sequence. First, we train a 3D vector-quantized variational autoencoder (VQVAE), which maps 3D objects into a discrete latent space to achieve efficient and accurate shape representation 
                and reconstruction. Building upon the 3D-aware discrete tokens, we innovatively construct a large-scale continuous training dataset named 3D-Alpaca, encompassing generation, comprehension, and editing, thus providing 
                rich resources for future research and training. Finally, by performing instruction-based training of the Qwen-2.5-vl-7B-Instruct model on the 3D-Alpaca dataset. Our work provides an effective attempt at extending multimodal 
                models with basic 3D capabilities, which contributes to future research in 3D-native AI.
                </p>
            </div>

            <!-- 父容器使用 flex 布局，将子元素水平排列 -->
             <h2 class="title is-3" style="text-align: center;">Method</h2>
            <div style="display: flex; gap: 1rem;">
            <!-- 第一个图片和描述 -->
            <div style="flex: 1; text-align: center;">
                <img src="./static/images/framework.png" alt="ShapeLLM-Omni" style="width: 100%; height: auto;" />
                <p style="margin-top: 0.5rem;">
                <strong>Fig 1. the model architecture of ShapeLLM-Omni</strong>. ShapeLLM-Omni inherits Qwen2.5-vl’s strong multimodal capabilities 
                and additionally supports text-to-3D, image-to-3D, 3D captioning, and 3D editing using text instruction.
                </p>
            </div>

            <!-- 第二个图片和描述 -->
            <div style="flex: 1; text-align: center;">
                <img src="./static/images/dataset.png" alt="3D-Alpaca dataset" style="width: 100%; height: auto;" />
                <p style="margin-top: 0.5rem;">
                <strong>Fig 2. overview of our constructed 3D-Alpaca dataset</strong>. Our proposed 3D-Alpaca dataset comprises 3D generation, 3D understanding, and 
                3D editing components, providing a comprehensive foundation for training and evaluating 3D largelanguage models.
                </p>
            </div>
            </div>

            <h2 class="title is-3" style="text-align: center;">Qualitative result</h2>
            <div class="column content">
            <video id="matting-video" autoplay muted controls loop playsinline height="100%">
              <source src="static/images/text.mp4"
                      type="video/mp4">
            </video>
            <p style="text-align: center;">
                <strong>Some <b>text-to-3D<b> examples</strong>.
            </p>

            <div class="column content">
            <video id="matting-video" autoplay muted controls loop playsinline height="100%">
              <source src="static/images/image2.mp4"
                      type="video/mp4">
            </video>
            <p style="text-align: center;">
              <strong>Some <b>image-to-3D</b> examples</strong>.
            </p>

            <h2 class="title is-3" style="text-align: center;">Demo Example</h2>
                <video id="matting-video" autoplay muted controls loop playsinline height="100%">
      	        <source src="static/images/open_video5.mp4" type="video/mp4"></video>
            <div class="content has-text-justified">
                <p>
                <strong>Fig 7. an example of our demo</strong>. We offer a demo showcasing our image-to-3D, text-to-3D, and 3D understanding capabilities. Please feel free to try it!
                </p>
            </div>
        </div>
        <!-- <h2 class="title is-3" style="text-align: center;"> Illustration Video </h2>
        <div class="columns is-centered has-text-centered">
          
          <div class="column is-full-width">
            <div class="column">
              <video id="teaser" autoplay muted loop playsinline width="1350" height="600">
                <source src="video.mp4" type="video/mp4">
              </video>
              <p>
                <strong>Illustration of DeepMesh.</strong> DeepMesh generates high-quality meshes from raw point clouds. It can also refine existing meshes, improving their structure and quality.
              </p>
            </div>
          </div>
        </div> -->
        
    </section>
   
    
    
    
   

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title is-4">BibTeX</h2>
            <pre>
                <code>
@article{ye2025shapellm,
  title={ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding},
  author={Ye, Junliang and Wang, Zhengyi and Zhao, Ruowen and Xie, Shenghao and Zhu, Jun},
  journal={arXiv preprint arXiv:2506.01853},
  year={2025}
}
                </code>
            </pre>
        </div>
    </section>

    


    

</body>

</html>
