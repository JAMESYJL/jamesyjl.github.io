<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Junliang Ye</title>
  
  <meta name="author" content="Junliang Ye">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Junliang Ye | 叶俊良</name>
              </p>
              <p> 
                I am a second-year master's student in the Department of <a href="https://www.cs.tsinghua.edu.cn/index.htm"> 
                  Computer Science </a> at <a href="https://www.tsinghua.edu.cn/en/"> Tsinghua University </a>, 
                  advised by Prof. <a href="https://ml.cs.tsinghua.edu.cn/~jun/index.shtml">Jun Zhu</a>. 
                  In 2022, I obtained my B.S. in the School of Mathematical Sciences at Peking University.
              </p>
              <p>
                My research interests lie in computer vision (e.g., 3D AIGC and video generation), multimodal large models (e.g., native large models), and reinforcement learning from human feedback (DPO, GRPO). 
              </p>
              <p>
                My email:yejl23@mails.tsinghua.edu.cn
              </p>
              <p style="text-align:center">
                <a href="mailto:yejl23@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://jamesyjl.github.io/">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=TKpuiuIAAAAJ&hl=en/"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/JAMESYJL"> Github </a>
              </p>
            </td>
            <td style="padding:3%;width:40%;max-width:40%">
              <img style="width:70%;max-width:70%" alt="profile photo" src="images/junliang1.jpg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
	      <li style="margin: 5px;" >
              <b>2025-06:</b> One papers on 3D Vision are accepted by <a href="https://iccv.thecvf.com/Conferences/2025/">ICCV 2025</a>.
              </li>
	      <li style="margin: 5px;" >
              <b>2024-07:</b> Two papers on 3D AIGC are accepted by <a href="https://eccv.ecva.net/">ECCV 2024</a>.
              </li>
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
		    
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Publications</heading></p>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/head.jpg" alt="dise">
            </td>
            <td width="80%" valign="center">
              <papertitle>ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding</papertitle>
              <br>
	      <strong>Junliang Ye*</strong>,
              Zhengyi Wang*,
              Ruowen Zhao*, 
	      Shenghao Xie,
	      Jun Zhu
              <br>
              <em>Arxiv</em>2025
              <br>
              <a href="https://arxiv.org/abs/2506.01853">[arXiv]</a>
              <a href="https://github.com/JAMESYJL/ShapeLLM-Omni/tree/main">[Code]</a>
              <a href="https://jamesyjl.github.io/ShapeLLM/">[Project Page]</a> 
	      <a href="https://github.com/JAMESYJL/ShapeLLM-Omni/">
		      <img src="https://img.shields.io/github/stars/JAMESYJL/ShapeLLM-Omni?style=social" style="vertical-align: middle;">
	      </a >
              <br>
              <p> We propose ​ShapeLLM-Omni, a multimodal large model that integrates ​3D generation, understanding, and editing capabilities.</p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/shp.jpeg" alt="dise">
            </td>
            <td width="80%" valign="center">
              <papertitle>DeepMesh: Auto-Regressive Artist-Mesh Creation With Reinforcement Learning</papertitle>
              <br>
              Ruowen Zhao*, 
              <strong>Junliang Ye*</strong>,
              Zhengyi Wang*,
	      Gaungce Liu,
              Yiwen Chen,
	      Yikai Wang,
	      Jun Zhu
              <br>
              <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2503.15265">[arXiv]</a>
              <a href="https://github.com/zhaorw02/DeepMesh/tree/main">[Code]</a>
              <a href="https://zhaorw02.github.io/DeepMesh/">[Project Page]</a> 
	      <a href="https://github.com/zhaorw02/DeepMesh/">
              <img src="https://img.shields.io/github/stars/zhaorw02/DeepMesh?style=social" style="vertical-align: middle;">
              </a >
              <br>
              <p> We propose DeepMesh, which generates meshes with intricate details and precise topology, 
		      surpassing state-of-the-art methods in both precision and quality.</p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/pr.png" alt="dise">
            </td>
            <td width="80%" valign="center">
              <papertitle>DreamReward-X: Boosting High-Quality 3D Generation with Human Preference Alignment</papertitle>
              <br>
              Fangfu Liu, 
              <strong>Junliang Ye</strong>,
	      Yikai Wang,
              Hanyang Wang,
              Zhengyi Wang,
	      Jun Zhu,
              Yueqi Duan
              <br>
              <em>Submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence, TPAMI</em>, 2024
              <br>
              <a>[arXiv]</a>
              <a>[Code]</a>
              <a>[Project Page]</a> 
              <br>
              <p> We present a comprehensive framework, coined DreamReward++, where we introduce a reward-aware noise sampling strategy 
		      to unleash text-driven diversity during the generation process while ensuring human preference alignment. Grounded 
		      by theoretical proof and extensive experiment comparisons, our method successfully generates high-fidelity and 
		      diverse 3D results with significant boosts in prompt alignment with human intention. 
		      Our results demonstrate the great potential for learning from human feedback to improve 3D generation.</p>
            </td>
          </tr>
	  
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/reconX.png" alt="dise">
            </td>
            <td width="80%" valign="center">
              <papertitle>ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model</papertitle>
              <br>
              Fangfu Liu*, 
              Wenqiang Sun*,
              Hanyang Wang*,
              Yikai Wang,
	      Sun Haowen,
              <strong>Junliang Ye</strong>,
	      Jun Zhang</a>,
              &nbsp;&nbsp;&nbsp;
              Yueqi Duan
              <br>
              <em>Arxiv</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2408.16767">[arXiv]</a>
              <a href="https://github.com/liuff19/ReconX">[Code]</a>
              <a href="https://liuff19.github.io/ReconX/">[Project Page]</a> 
	      <a href="https://github.com/liuff19/ReconX">
              <img src="https://img.shields.io/github/stars/liuff19/ReconX?style=social" style="vertical-align: middle;">
              </a >
              <br>
              <p>  In this paper, we propose ReconX, a novel 3D scene reconstruction paradigm that reframes 
		      the ambiguous reconstruction challenge as a temporal generation task. The key insight is to 
		      unleash the strong generative prior of large pre-trained video diffusion models 
		      for sparse-view reconstruction.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/pipeline.jpg" alt="dise">
            </td>
            <td width="80%" valign="center">
              <papertitle>DreamReward: Aligning Human Preference in Text-to-3D Generation</papertitle>
              <br>
              <strong>Junliang Ye*</strong>, 
              Fangfu Liu*, 
              Qixiu Li,
              Zhengyi Wang,
              Yikai Wang,
              Xinzhou Wang,
              Yueqi Duan </a>,
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              Jun Zhu
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
              <br>
              <a href="http://arxiv.org/abs/2403.14613">[arXiv]</a>
              <a href="https://github.com/liuff19/DreamReward">[Code]</a>
              <a href="https://jamesyjl.github.io/DreamReward/">[Project Page]</a> 
	      <a href="https://github.com/liuff19/DreamReward">
              <img src="https://img.shields.io/github/stars/liuff19/DreamReward?style=social" style="vertical-align: middle;">
              </a >
              <br>
              <p> We present a comprehensive framework, coined DreamReward, to 
                learn and improve text-to-3D models from human preference feedback.</p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/pipeline4.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>AnimatableDreamer: Text-Guided Non-rigid 3D Model Generation 
                and Reconstruction with Canonical Score Distillation</papertitle>
              <br>
              Xinzhou Wang,
              Yikai Wang,
              <strong>Junliang Ye</strong>, 
              Zhengyi Wang,
              Fuchun Sun,
              Pengkun Liu,
              Ling Wang,
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
              Kai Sun, 
              Xintong Wang, 
              Bin He
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2312.03795">[arXiv]</a>
              <a href="https://github.com/AnimatableDreamer/AnimatableDreamer">[Code]</a>
              <a href="https://animatabledreamer.github.io/">[Project Page]</a> 
	      <a href="https://github.com/AnimatableDreamer/AnimatableDreamer">
              <img src="https://img.shields.io/github/stars/AnimatableDreamer/AnimatableDreamer?style=social" style="vertical-align: middle;">
              </a >
              <br>
              <p> We propose ANIMATABLEDREAMER, a framework with the capability to generate generic categories of non-rigid 3D models.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/pipeline3-2.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>PKU_WICT at TRECVID 2022: Disaster Scene Description and Indexing Task</papertitle>
              <br>
              Yanzhe Chen, 
              HsiaoYuan Hsu,
              <strong>Junliang Ye</strong>, 
              Zhiwen Yang, 
              Zishuo Wang, 
              Xiangteng He, 
              Yuxin Peng
              <br>
              Virtual, Online
              <br>
              <a >[arXiv]</a>
              <a >[Code]</a>
              <a >[Project Page]</a> 
              <br>
              <p> We achieved first place in the TRECVID 2022 competition.</p>
            </td>
          </tr>

        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                Review for <b>CVPR 2023</b>.
              </li>
          </td>
        </tr>
      </tbody></table>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

<p><center>
	  <div id="clustrmaps-widget" style="width:5%">
      <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=B_hoqUcZkAVteexiuKv_tIvNw9enA1g2tIC3ypxXP2E"></script>
	  <br>
	    &copy; Junliang Ye | Last updated: 18 Mar, 2024
</center></p>
</body>

</html>
